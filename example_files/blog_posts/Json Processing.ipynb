{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import collections\n",
    "import json\n",
    "import ast, sys\n",
    "import urllib2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_duplicate_rows(df, column_name_to_filter):\n",
    "    grouped = df.groupby(column_name_to_filter)\n",
    "    index = [gp_keys[0] for gp_keys in grouped.groups.values()]\n",
    "    return df.reindex(index)\n",
    "\n",
    "def convert_dict_unicode2string(data):\n",
    "    if isinstance(data, basestring):\n",
    "        return data.encode(\"utf8\")\n",
    "    elif isinstance(data, collections.Mapping):\n",
    "        return dict(map(convert_dict_unicode2string, data.iteritems()))\n",
    "    elif isinstance(data, collections.Iterable):\n",
    "        return type(data)(map(convert_dict_unicode2string, data))\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "import mimetypes\n",
    "import random\n",
    "import string\n",
    "\n",
    "_BOUNDARY_CHARS = string.digits + string.ascii_letters\n",
    "\n",
    "def encode_multipart(fields, files, boundary=None):\n",
    "    r\"\"\"Encode dict of form fields and dict of files as multipart/form-data.\n",
    "    Return tuple of (body_string, headers_dict). Each value in files is a dict\n",
    "    with required keys 'filename' and 'content', and optional 'mimetype' (if\n",
    "    not specified, tries to guess mime type or uses 'application/octet-stream').\n",
    "\n",
    "    >>> body, headers = encode_multipart({'FIELD': 'VALUE'},\n",
    "    ...                                  {'FILE': {'filename': 'F.TXT', 'content': 'CONTENT'}},\n",
    "    ...                                  boundary='BOUNDARY')\n",
    "    >>> print('\\n'.join(repr(l) for l in body.split('\\r\\n')))\n",
    "    '--BOUNDARY'\n",
    "    'Content-Disposition: form-data; name=\"FIELD\"'\n",
    "    ''\n",
    "    'VALUE'\n",
    "    '--BOUNDARY'\n",
    "    'Content-Disposition: form-data; name=\"FILE\"; filename=\"F.TXT\"'\n",
    "    'Content-Type: text/plain'\n",
    "    ''\n",
    "    'CONTENT'\n",
    "    '--BOUNDARY--'\n",
    "    ''\n",
    "    >>> print(sorted(headers.items()))\n",
    "    [('Content-Length', '193'), ('Content-Type', 'multipart/form-data; boundary=BOUNDARY')]\n",
    "    >>> len(body)\n",
    "    193\n",
    "    \"\"\"\n",
    "    def escape_quote(s):\n",
    "        return s.replace('\"', '\\\\\"')\n",
    "\n",
    "    if boundary is None:\n",
    "        boundary = ''.join(random.choice(_BOUNDARY_CHARS) for i in range(30))\n",
    "    lines = []\n",
    "\n",
    "    for name, value in fields.items():\n",
    "        lines.extend((\n",
    "            '--{0}'.format(boundary),\n",
    "            'Content-Disposition: form-data; name=\"{0}\"'.format(escape_quote(name)),\n",
    "            '',\n",
    "            str(value),\n",
    "        ))\n",
    "\n",
    "    for name, value in files.items():\n",
    "        filename = value['filename']\n",
    "        if 'mimetype' in value:\n",
    "            mimetype = value['mimetype']\n",
    "        else:\n",
    "            mimetype = mimetypes.guess_type(filename)[0] or 'application/octet-stream'\n",
    "        lines.extend((\n",
    "            '--{0}'.format(boundary),\n",
    "            'Content-Disposition: form-data; name=\"{0}\"; filename=\"{1}\"'.format(\n",
    "                    escape_quote(name), escape_quote(filename)),\n",
    "            'Content-Type: {0}'.format(mimetype),\n",
    "            '',\n",
    "            value['content'],\n",
    "        ))\n",
    "\n",
    "    lines.extend((\n",
    "        '--{0}--'.format(boundary),\n",
    "        '',\n",
    "    ))\n",
    "    body = '\\r\\n'.join(lines)\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'multipart/form-data; boundary={0}'.format(boundary),\n",
    "        'Content-Length': str(len(body)),\n",
    "    }\n",
    "\n",
    "    return (body, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting output from \"Session Buddy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data.json') as f:\n",
    "    string = \"\"\n",
    "    for line in f:\n",
    "        string += line\n",
    "        \n",
    "data = json.loads(string)\n",
    "df = pd.DataFrame(data[\"tabs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_df = df[['title','url']]\n",
    "output_df.insert( 2,'user', \"dataradar\")\n",
    "output_df.insert( 3,'tags',\"\")\n",
    "output_df.insert( 4,'twitter',True)\n",
    "output_df.insert( 5,'autoformat',False)\n",
    "output_df.insert( 6,'activated',True)\n",
    "\n",
    "output_df = remove_duplicate_rows(output_df, 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_df.to_json(\"output.json\", orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Uniqueness in consolidated.json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('consolidated.json') as f:\n",
    "    string = \"\"\n",
    "    for line in f:\n",
    "        string += line\n",
    "        \n",
    "data = json.loads(string)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = remove_duplicate_rows(df, 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api_route = \"https://www.feedcrunch.io/api/1.0/authenticated/get/article/exists/\"\n",
    "\n",
    "apikey_local = \"91ce86bd-ae46-4250-b599-727c1970b609\"\n",
    "apikey_fc    = \"4377190f-24a5-4ef7-a761-a877bf6218e3\"\n",
    "\n",
    "api_url = api_route + apikey_fc + \"/\"\n",
    "\n",
    "def check_link_exists(url, user):\n",
    "    r = requests.get(api_url+'?link='+url+'&posting_user='+user)\n",
    "    try:\n",
    "        return r.json()[\"exists\"]\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines_2_delete = []\n",
    "\n",
    "for index in range(df.shape[0]):\n",
    "    \n",
    "    url = df.iloc[index]['url']\n",
    "    user = df.iloc[index]['user']\n",
    "    \n",
    "    if check_link_exists(url, user):\n",
    "        lines_2_delete.append(index)\n",
    "        print 'Duplicate Found: ' + url + \" && user: \" + user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop(df.index[lines_2_delete], inplace=True)\n",
    "\n",
    "df = df.sort_values(by=['url', 'title'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_json(\"consolidated.json\", orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('consolidated.json') as f:\n",
    "    string = \"\"\n",
    "    for line in f:\n",
    "        string += line\n",
    "        \n",
    "data = json.loads(string)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print \"There are \" + str(df.shape[0]) + \" articles in dataset\"\n",
    "df = remove_duplicate_rows(df, 'url')\n",
    "print \"There are \" + str(df.shape[0]) + \" articles in dataset without duplicates\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sorting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('consolidated.json') as f:\n",
    "    string = \"\"\n",
    "    for line in f:\n",
    "        string += line\n",
    "        \n",
    "data = json.loads(string)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = remove_duplicate_rows(df, 'link') \n",
    "df = df.sort_values(by=['link', 'title'], ascending=[True, True])\n",
    "#df = df.sort_values(by=['title', 'url'], ascending=[True, True])\n",
    "\n",
    "df.to_json(\"consolidated.json\", orient=\"records\", force_ascii=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hashtag Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_hashtags = 5\n",
    "\n",
    "def hashtags_free_number(df, index):\n",
    "    \n",
    "    tags = df.iloc[index]['tags']\n",
    "        \n",
    "    str_list = tags.split(\",\")\n",
    "    str_list = filter(None, str_list) # fastest\n",
    "    \n",
    "    return max_hashtags - len(str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_substr(input_str, keyword):\n",
    "    \n",
    "    tmp_str = str.lower(input_str)   \n",
    "    tmp_keyword = str.lower(keyword)\n",
    "    \n",
    "    if keyword[-1:] == '-':\n",
    "        \n",
    "        tmp_str = str.replace(tmp_str, ' ', '')\n",
    "        tmp_str = str.replace(tmp_str, '-', '')\n",
    "        tmp_str = str.replace(tmp_str, '_', '')\n",
    "        tmp_str = str.replace(tmp_str, '/', '')\n",
    "        tmp_str = str.replace(tmp_str, '#', '')\n",
    "        tmp_str = str.replace(tmp_str, '?', '')\n",
    "\n",
    "        \n",
    "        tmp_keyword = str.replace(tmp_keyword, ' ', '')\n",
    "        tmp_keyword = str.replace(tmp_keyword, '-', '')\n",
    "        tmp_keyword = str.replace(tmp_keyword, '_', '')\n",
    "        tmp_keyword = str.replace(tmp_keyword, '/', '')\n",
    "        tmp_keyword = str.replace(tmp_keyword, '#', '')\n",
    "        tmp_keyword = str.replace(tmp_keyword, '?', '')\n",
    "    \n",
    "    \n",
    "    if tmp_keyword in tmp_str:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_pattern(df, index, pattern):\n",
    "    \n",
    "    if check_substr(df.iloc[index]['title'].encode(\"utf-8\"), pattern):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_keyword(df, index, keyword_list, pattern = \"\"):\n",
    "     \n",
    "    if not check_pattern(df, index, pattern):\n",
    "        return False\n",
    "        \n",
    "    tags = df.iloc[index]['tags']\n",
    "        \n",
    "    str_list = tags.split(\",\")\n",
    "    str_list = filter(None, str_list) # fastest\n",
    "    \n",
    "    str_list = [s.encode('ascii') for s in str_list] # unicode to string\n",
    "    \n",
    "    old_tmp = str_list[:]\n",
    "    \n",
    "    for keyword in keyword_list:\n",
    "        if len(str_list) >= max_hashtags:\n",
    "            break\n",
    "        if keyword not in str_list:\n",
    "            str_list.append(keyword)\n",
    "    \n",
    "    df.set_value(index, \"tags\", \",\".join(str_list))\n",
    "    return max_hashtags - len(str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_stats(df, posts_full, full_at_start):\n",
    "    data = []\n",
    "    total_posts = df.shape[0]\n",
    "    print \"Posts Full = \" + str(posts_full)+\"/\"+str(df.shape[0])\n",
    "    print \"Posts Full at Start = \" + str(full_at_start)+\"/\"+str(df.shape[0])\n",
    "\n",
    "    for i in range (100):\n",
    "        data.append(0)\n",
    "\n",
    "    for index in range(df.shape[0]):\n",
    "        tags = df.iloc[index]['tags']\n",
    "\n",
    "        str_list = tags.split(\",\")\n",
    "        str_list = filter(None, str_list) # fastest\n",
    "\n",
    "        data[len(str_list)] += 1\n",
    "\n",
    "    for i in range (100):\n",
    "        if data[i] != 0:\n",
    "            print \"Number of rows with \" + str(i) + \" Hashtags : \" + str(data[i]) + \"/\" + str(total_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################### LOADING JSON ##################################\n",
    "with open('consolidated.json') as f:\n",
    "    string = \"\"\n",
    "    for line in f:\n",
    "        string += line\n",
    "        \n",
    "data = json.loads(string)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = remove_duplicate_rows(df, 'link') \n",
    "df = df.sort_values(by=['link', 'title'], ascending=[True, True])\n",
    "\n",
    "###################### PARSING JSON #################################\n",
    "posts_full = 0\n",
    "full_at_start = 0\n",
    "\n",
    "for index in range(df.shape[0]):\n",
    "    max_iteration = hashtags_free_number(df, index)\n",
    "    \n",
    "    if max_iteration > 0:\n",
    "        for pattern, keywords_list in keywords.iteritems():  \n",
    "            tmp_rslt = set_keyword(df, index, keywords_list, pattern)\n",
    "            \n",
    "            if tmp_rslt is not False:\n",
    "                max_iteration = tmp_rslt\n",
    "            \n",
    "                if max_iteration <= 0:\n",
    "                    break\n",
    "        \n",
    "        #\"\"\"\n",
    "        if max_iteration > 0 and df.iloc[index]['user'] == \"dataradar\":\n",
    "            \n",
    "            Last_Keywords = ['Data', 'DataScience','AI']            \n",
    " \n",
    "            tmp_rslt = set_keyword(df, index, Last_Keywords, \"\")\n",
    "\n",
    "            if tmp_rslt is not False:\n",
    "                max_iteration = tmp_rslt\n",
    "        #\"\"\"\n",
    "                        \n",
    "        if max_iteration <= 0:\n",
    "            posts_full += 1\n",
    "        elif max_iteration == 3:\n",
    "            print df.iloc[index]['title'] \n",
    "            print df.iloc[index]['tags']\n",
    "            print \"###########\"\n",
    "        \n",
    "    else:\n",
    "        full_at_start += 1\n",
    "        posts_full += 1\n",
    "\n",
    "write_stats(df, posts_full, full_at_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_json(\"consolidated.json\", orient=\"records\", force_ascii=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keywords = {\n",
    "    # DataRadar Oriented Tags \n",
    "    'Accuracy': ['Accuracy'],\n",
    "    'Adversa': ['GAN', 'DeepLearning', 'MachineLearning', 'NeuralNet'],\n",
    "    'Analysis': ['Analysis'],\n",
    "    'Analytic': ['Analytic'],\n",
    "    'Arxiv': ['Arxiv', 'Research'],\n",
    "    'AutoEncoder': ['AE', 'AutoEncoder', 'DeepLearning', 'MachineLearning', 'NeuralNet'],\n",
    "    'Bayes': ['Bayes', 'MachineLearning', 'Statistic'],\n",
    "    'BigData-': ['BigData'],\n",
    "    'Boosting': ['Boosting', 'MachineLearning'],\n",
    "    'CNN': ['CNN', 'ConvNet', 'DeepLearning', 'MachineLearning', 'NeuralNet'],\n",
    "    'CUDA': ['CUDA', 'GPU', 'NVidia'],\n",
    "    'Caffe': ['Caffe', 'DeepLearning', 'MachineLearning', 'NeuralNet', 'Library'],\n",
    "    'Cassandra': ['Cassandra', 'Apache', 'BigData'],\n",
    "    'Classifi': ['Classification', 'MachineLearning'],\n",
    "    'Clustering': ['Clustering', 'MachineLearning'],\n",
    "    'Cohort': ['Analysis', 'Cohort', 'Statistic'],\n",
    "    'ComputerVision-': ['ComputerVision', 'MachineLearning'],\n",
    "    'ConvNet': ['CNN', 'ConvNet', 'DeepLearning', 'MachineLearning', 'NeuralNet'],\n",
    "    'Convol': ['CNN', 'ConvNet', 'DeepLearning', 'MachineLearning', 'NeuralNet'],\n",
    "    'CrossValidation-': ['CrossValidation', 'Statistic', 'MachineLearning'],\n",
    "    'CuDNN': ['CuDNN', 'CUDA', 'GPU', 'NVidia'],\n",
    "    'DNN': ['DNN', 'DeepLearning', 'MachineLearning', 'NeuralNet'],\n",
    "    'DSSTNE': ['Amazon', 'DSSTNE', 'DeepLearning', 'MachineLearning', 'NeuralNet'],\n",
    "    'Data': ['Data', 'DataScience'],\n",
    "    'DataAnalysis-': ['DataAnalysis', 'MachineLearning', 'Statistic'],\n",
    "    'DataAnalyst-': ['DataScientist', 'DataAnalyst', 'Job'],\n",
    "    'DataLake-': ['DataLake', 'BigData'],\n",
    "    'DataScience-': ['DataScience', 'Data'],\n",
    "    'DataScientist-': ['DataScientist', 'DataAnalyst', 'Job'],\n",
    "    'DataStructure-': ['DataStructure'],\n",
    "    'DataViz-': ['DataViz'],\n",
    "    'Dataset': ['Dataset'],\n",
    "    'DeepDream-': ['DeepDream', 'Google', 'DeepLearning', 'MachineLearning', 'NeuralNet'],\n",
    "    'DeepLearning-': ['DeepLearning', 'MachineLearning', 'NeuralNet', 'Library'],\n",
    "    'Diagram': ['diagram', 'DataViz'],\n",
    "    'Distribution': ['Statistic', 'Probability', 'Distribution'],\n",
    "    'ElasticSearch-': ['ElasticSearch', 'BigData'],\n",
    "    'EnsembleLearning-': ['EnsembleLearning', 'MachineLearning'],\n",
    "    'EnsembleModel-': ['EnsembleLearning', 'MachineLearning'],\n",
    "    'FPGA': ['FPGA'],\n",
    "    'FaceDetection-': ['FaceDetection', 'ComputerVision'],\n",
    "    'FaceRecognition-': ['FaceRecognition', 'ComputerVision'],\n",
    "    'FeatureExtract-': ['FeatureExtraction', 'MachineLearning'],\n",
    "    'FeatureSelect-': ['FeatureSelection', 'MachineLearning'],\n",
    "    'Flink': ['Flink', 'Apache', 'BigData'],\n",
    "    'GAN': ['GAN', 'DeepLearning', 'MachineLearning', 'NeuralNet'],\n",
    "    'Generali': ['Generalisation', 'MachineLearning'],\n",
    "    'GenerativeModel-': ['GenerativeModel', 'MachineLearning'],\n",
    "    'GettingStarted-': ['GettingStarted'],\n",
    "    'Gradient': ['Gradient', 'MachineLearning'],\n",
    "    'GradientBoosting-': ['GradientBoosting', 'MachineLearning'],\n",
    "    'GradientDescent-': ['GradientDescent', 'MachineLearning'],\n",
    "    'Guide': ['Guide'],\n",
    "    'Hadoop': ['Hadoop', 'Apache', 'BigData'],\n",
    "    'HowTo-': ['HowTo', 'GettingStarted'],\n",
    "    'HyperParam-': ['HyperParameter', 'MachineLearning'],\n",
    "    'ImageAnaly-': ['ImageAnalysis', 'ComputerVision', 'MachineLearning'],\n",
    "    'ImageCaptio-': ['ImageCaptioning', 'ComputerVision', 'MachineLearning'],\n",
    "    'ImageClassifi-': ['ImageClassification', 'ComputerVision', 'MachineLearning'],\n",
    "    'ImageProcess-': ['ImageProcessing', 'ComputerVision', 'MachineLearning'],\n",
    "    'ImageRecogni-': ['ImageRecognition', 'ComputerVision', 'MachineLearning'],\n",
    "    'ImageSegment-': ['ImageSegmentation', 'ComputerVision', 'MachineLearning'],\n",
    "    'ImageUnderst-': ['ImageUnderstanding', 'ComputerVision', 'MachineLearning'],\n",
    "    'Kafka': ['Kafka', 'Apache', 'Distributed', \"Streaming\"],\n",
    "    'KMeans': ['KMeans', 'MachineLearning', 'Clustering'],\n",
    "    'KNN': ['KNN', 'MachineLearning', 'Classification'],\n",
    "    'Kaggle': ['Competition', 'Kaggle', 'MachineLearning'],\n",
    "    'Keras': ['DeepLearning', 'Keras', 'MachineLearning', 'NeuralNet', 'Library'],\n",
    "    'LDA': ['LDA', 'MachineLearning', 'FeatureSelection'],\n",
    "    'LSTM': ['DeepLearning', 'LSTM', 'MachineLearning', 'RNN', 'NeuralNet'],\n",
    "    'Lasagne': ['DeepLearning', 'Lasagne', 'MachineLearning', 'NeuralNet', 'Library'],\n",
    "    'Mac': ['Mac', 'MacOS', 'Apple'],\n",
    "    'MNIST': ['MNIST'],\n",
    "    'MXNet': ['DeepLearning', 'MXNet', 'MachineLearning', 'NeuralNet', 'Library'],\n",
    "    'MachineLearning-': ['MachineLearning'],\n",
    "    'MariaDB': ['Database', 'MariaDB'],\n",
    "    'Markov': ['MarkovChain', 'MachineLearning', 'NeuralNet'],\n",
    "    'Mathemati': ['Mathematic', 'Science'],\n",
    "    'MissingVal-': ['MissingValue'],\n",
    "    'Model': ['Model'],\n",
    "    'MongoDB': ['Database', 'MongoDB'],\n",
    "    'MonteCarlo-': ['MonteCarlo', 'Stochastic'],\n",
    "    'Multilayer-': [' MultiLayer', 'DeepLearning', 'NeuralNet'],\n",
    "    'MultiClass-': ['MultiClass', 'MachineLearning'],\n",
    "    'MultiLabel-': ['MultiClass', 'MachineLearning'],\n",
    "    'MySQL': ['Database', 'MySQL'],\n",
    "    'NLP': ['NLP', 'DeepLearning', 'MachineLearning'],\n",
    "    'NVidia': ['NVidia'],\n",
    "    'Neo4J': ['Neo4J', 'Database', 'GraphDB'],\n",
    "    'Neural': ['NeuralNet', 'DeepLearning', 'MachineLearning'],\n",
    "    'NeuralNet-': ['NeuralNet', 'DeepLearning', 'MachineLearning'],\n",
    "    'NoSQL': ['NoSQL', 'Database', 'BigData'],\n",
    "    'NonLinear-': ['NonLinear', 'MachineLearning'],\n",
    "    'Numpy': ['Numpy', 'Python', 'Library'],\n",
    "    'ObjectDetec-': ['ObjectDetection', 'ComputerVision', 'DeepLearning', 'MachineLearning'],\n",
    "    'ObjectReco-': ['ObjectRecognition', 'ComputerVision', 'DeepLearning', 'MachineLearning'],\n",
    "    'ObjectSegmen-': ['ObjectSegmentation', 'ComputerVision', 'DeepLearning', 'MachineLearning'],\n",
    "    'Pandas': ['Pandas', 'Python', 'Library', 'DataManipulation'],\n",
    "    'Pipeline': ['DataPipeline', 'MachineLearning'],\n",
    "    'PostgreSQL': ['Database', 'PostgreSQL'],\n",
    "    'Proba': ['Probability', 'Statistic'],\n",
    "    'Propaga': ['BackPropagation', 'DeepLearning', 'NeuralNet'],\n",
    "    'Python': ['Python'],\n",
    "    'RSS': ['RSS', 'Feed'],\n",
    "    'Recurrent': ['DeepLearning', 'MachineLearning', 'RNN', 'NeuralNet'],\n",
    "    'SVM': ['MachineLearning', 'SVM', 'Classification'],\n",
    "    'Scikit': ['Python', 'Scikit', 'MachineLearning', 'Library'],\n",
    "    'Scipy': ['Python', 'Scipy', 'MachineLearning', 'Library'],\n",
    "    'Segmentation': [\"Segmentation\"],\n",
    "    'SelectFeatu-': ['FeatureSelection', 'MachineLearning'],\n",
    "    'Sentiment': ['SentimentAnalysis', 'NLP', 'MachineLearning'],\n",
    "    'Spark': ['Apache', 'Spark', 'BigData'],\n",
    "    'Streaming': ['Streaming', 'MachineLearning'],\n",
    "    'TensorFlow': ['TensorFlow', 'DeepLearning', 'MachineLearning', 'NeuralNet', 'Library'],\n",
    "    'Tensor': ['TensorFlow', 'DeepLearning', 'MachineLearning', 'NeuralNet', 'Library'],\n",
    "    'TextMining-': ['TextMining', 'MachineLearning'],\n",
    "    'TextProcess-': ['TextProcessing', 'MachineLearning'],\n",
    "    'Theano': ['DeepLearning', 'MachineLearning', 'Theano', 'Library'],\n",
    "    'Tutorial': ['Tutorial', 'GettingStarted'],\n",
    "    'Visuali': ['DataViz', 'MachineLearning'],\n",
    "    'Vision': ['ComputerVision', 'MachineLearning'],\n",
    "     \n",
    "    # Engineering Oriented Tags \n",
    "    '2D': [\"2D\"],\n",
    "    '3D': ['3D'],\n",
    "    '3DModel-': ['3DModel', '3D'],\n",
    "    'ABTest-': ['ABTest'],\n",
    "    'API': ['API'],\n",
    "    'AWS': ['AWS', 'Amazon', 'Cloud'],\n",
    "    'AWSLambda-': ['AWSLambda', 'ServerLess', 'AWS', 'Amazon', 'Cloud'],\n",
    "    'Academic': ['Academic', 'Research'],\n",
    "    'Administration': ['Administration'],\n",
    "    'AddOn-': ['AddOn'],\n",
    "    'Adventure': [\"Adventure\"],\n",
    "    'Agile': ['Agile'],\n",
    "    'Algorithm': ['Algorithm', 'ComputerScience'],\n",
    "    'Amazon': ['Amazon'],\n",
    "    'AmazonWebService-': ['AWS', 'Amazon', 'Cloud', 'SaaS', 'PaaS'],\n",
    "    'Android': ['Android', 'Mobile', 'Smartphone'],\n",
    "    'Angular': ['Angular', 'JS', 'WebApp'],\n",
    "    'Animation': ['Animation', 'CSS', 'WebDevelopment', 'WebApp'],\n",
    "    'Approach': ['Approach'],\n",
    "    'Async': ['Async'],\n",
    "    'AutoScaling-': ['AutoScaling'],\n",
    "    'Automation': ['Automation'],\n",
    "    'Backdoor': ['Backdoor', 'Security', 'ITSec', 'InfoSec'],\n",
    "    'Backup': ['Backup'],\n",
    "    'Bash': ['Bash', 'Script', 'Linux'],\n",
    "    'BeautifulSoup-': ['BeautifulSoup', 'Python'],\n",
    "    'Behavior': ['Behavior'],\n",
    "    'Benchmark': ['Benchmark'],\n",
    "    'Botnet': ['Botnet', 'ITSecurity', 'InfoSec'],\n",
    "    'Browser': [\"Browser\"],\n",
    "    'Business': ['Business'],\n",
    "    'Code': ['Code', 'DevLife'],\n",
    "    'C++': ['C++'],\n",
    "    'CPU': ['CPU'],\n",
    "    'CSS': ['CSS', 'WebApp'],\n",
    "    'CSS3': ['CSS3'],\n",
    "    'Cache': ['Caching'],\n",
    "    'Caching': ['Caching'],\n",
    "    'Certificate': ['Certificate', 'Security', 'Encryption', 'ITSec', 'InfoSec'],\n",
    "    'ChatBot-': ['ChatBot'],\n",
    "    'Cheatsheet': ['Cheatsheet'],\n",
    "    'CleanEnergy-': [\"CleanEnergy\", \"GreenEnergy\"],\n",
    "    'CodeEditor-': ['CodeEditor','Code', 'DevLife'],\n",
    "    'Collab': ['Collaborative'],\n",
    "    'CommandLine-': ['CommandLine'],\n",
    "    'Commit': ['Commit'],\n",
    "    'Compensation': ['Compensation', 'Gratification', 'Job', 'Salary'],\n",
    "    'Computing': ['Computing', 'ComputerScience'],\n",
    "    'Container': ['Container'],\n",
    "    'Control': ['Control'],\n",
    "    'CPython': ['CPython', 'Python', 'Performance', 'Optimization'],\t \n",
    "    'Crypto': ['Cryptography', 'Security', 'Encryption'],\n",
    "    'Customer': [\"Customer\"],\n",
    "    'Dashboard': [\"Dashboard\"],\n",
    "    'Decorator': ['Decorator', 'Code'],\n",
    "    'DesignPattern-': ['DesignPattern', \"Code\", 'DevLife', 'BestPratice', 'SoftwareArchitecture'],\n",
    "    'Design': ['Design'],\n",
    "    'Developer': ['Developer', 'Job'],\n",
    "    'Development': ['Development'],\n",
    "    'DevelopmentCycle-': [\"DevelopmentCycle\"],\n",
    "    'Django': ['Django', 'Python', 'WebApp', 'ORM'],\n",
    "    'Docker': ['Docker', 'Container', 'Virtualisation', 'DataCenter', 'VM'],\n",
    "    'Document': ['Document'],\n",
    "    'Encrypt': ['Encryption', 'Security', 'ITSec', 'InfoSec'],\n",
    "    'Energy': [\"Energy\"],\n",
    "    'Excel': ['Excel', 'SpreadSheet', 'Microsoft', 'Office'],\n",
    "    'Expert': [\"Expert\"],\n",
    "    'Facebook': ['Facebook'],\n",
    "    'Framework': ['Framework'],\n",
    "    'Fundamental': ['Fundamental'],\n",
    "    'FundRaising': ['FundRaising'],\n",
    "    'Funding': ['Funding'],\n",
    "    'GCE': ['GCE', 'Google', 'Cloud'],\n",
    "    'GCP': ['GCP', 'Google', 'Cloud'],\n",
    "    'GPU': ['GPU'],\n",
    "    'Galler': ['Gallery', 'Photo', 'Image'],\n",
    "    'Git': ['Git', 'OpenSource'],\n",
    "    'Github': ['Github', 'Git', 'OpenSource'],\n",
    "    'Gitlab': ['Gitlab', 'Git', 'OpenSource'],\n",
    "    'GoldenRule': ['GoldenRule'],\n",
    "    'Google': ['Google'],\n",
    "    'GoogleCloudPlatform-': ['GCP', 'Google', 'Cloud', 'SaaS', 'PaaS'],\n",
    "    'GrowthHacking-': ['GrowthHacking'],\n",
    "    'Hash': [\"Hashing\", 'Hash', 'Cryptography', \"ITSec\", \"InfoSec\"],\n",
    "    'HTML': ['HTML', 'WebApp'],\n",
    "    'HTML5': [\"HTML5\"],\n",
    "    'HTTPS': ['HTTPS', 'Encryption', 'Security'],\n",
    "    'Handbook': ['Handbook'],\n",
    "    'Haskell': ['Haskell'],\n",
    "    'HumanCost-': [\"HumanCost\"],\n",
    "    'IPython': ['IPython', 'Notebook', 'Python'],\n",
    "    'Industry': ['Industry'],\n",
    "    'Interactive': ['Interactive'],\n",
    "    'InternetOfThi-': ['IoT'],\n",
    "    'Interview': ['Interview'],\n",
    "    'IoT': ['IoT'],\n",
    "    'Introduc': ['Introduction'],\t \n",
    "    'JQuery': ['JQuery', 'JS', 'WebApp'],\n",
    "    'JS': ['JS', 'WebApp'],\n",
    "    'JSON': ['JSON'],\n",
    "    'Java': ['Java'],\n",
    "    'Javascript': ['JS', 'WebApp'],\n",
    "    'Jupyter': ['Jupyter', 'Notebook', 'Python'],\n",
    "    'Kubernetes': ['Kubernetes', 'Docker','Orchestration'],\n",
    "    'Language': [\"Language\"],\n",
    "    'LaTeX': ['LaTeX'],\n",
    "    'Log': [\"Log\"],\n",
    "    'MOOC': ['MOOC', 'Learning'],\n",
    "    'MVP': ['MVP', 'Startup'],\n",
    "    'MagicLeap-': ['MagicLeap', 'VR', 'AR'],\n",
    "    'MagicMirror-': ['MagicMirror'],\n",
    "    'Markup': ['Markup'],\n",
    "    'Memory': ['Memory', 'Performance'],\n",
    "    'Messag': ['Message'],\n",
    "    'Metasploit': ['Metasploit', 'Hack', 'Security', 'ITSec', 'InfoSec'],\n",
    "    'Microservice-': ['MicroService', 'ServerLess', 'Cloud', 'SaaS'],\n",
    "    'Microsoft': ['Microsoft'],\n",
    "    'Migration': ['Migration', 'Database', 'ORM'],\n",
    "    'Monitor': ['Monitoring', 'Service'],\n",
    "    'Motion': ['Motion'],\n",
    "    'NodeJS': ['NodeJS', 'JS', 'WebApp'],\n",
    "    'Notebook': ['Notebook', 'Notebook', 'Python'],\n",
    "    'Notif': ['Notification'],\n",
    "    'OAuth': ['OAuth', 'Authentication'],\n",
    "    'Online': ['Online'],\n",
    "    'OpenCV': ['OpenCV', 'ComputerVision', 'C++', 'Python'],\n",
    "    'OpenGL': ['OpenGL'],\n",
    "    'OpenID': ['OpenID', 'Authentication'],\n",
    "    'OpenSource-': ['OpenSource', 'Software'],\n",
    "    'Opportunit': [\"Opportunity\"],\n",
    "    'Orchestration': [\"Orchestration\"],\n",
    "    'Personal': ['Personal'],\n",
    "    'PHP': ['PHP', 'WebApp'],\n",
    "    'Pass': ['Password', 'Authentication'],\n",
    "    'Performance': ['Performance'],\n",
    "    'Pitch': ['Pitch', 'Startup', 'ElevatorPitch'],\n",
    "    'Process': ['Process'],\n",
    "    'Productiv': ['Productivity', 'Efficiency', 'Performance'],\n",
    "    'Product ': [\"Product\"], # Don't remove the space\n",
    "    'Programmer': ['Programmer', 'DevLife', 'ComputerScience'],\n",
    "    'Proof': ['Proof'],\n",
    "    'Publish': ['Publishing'],\n",
    "    'Push': ['Push'],\n",
    "    'Radiobox': [\"Radiobox\"],\n",
    "    'Reddit': [\"Reddit\"], \n",
    "    'REST': ['REST'],\n",
    "    'RSS': ['RSS', 'Feed'],\n",
    "    'Random': ['Random'],\n",
    "    'Raspberry': ['RaspberryPi', 'CheapComputing', 'EmbeddedDevice'],\n",
    "    'React': ['React', 'ReactJS', 'Javascript', 'WebApp', 'ReactNative'],\n",
    "    'Recommendation': ['Recommendation', 'RecommendationEngine', 'RecommenderSystem'],\n",
    "    'RegEx': ['Regex'],\n",
    "    'RegularExpr-': ['Regex'],\n",
    "    'RemoteAccess': ['RemoteAccess', 'RDP'],\n",
    "    'Render': ['Render'],\n",
    "    'Resilient': ['Resilient'],\n",
    "    'SAML': ['SAML', 'Authentication'],\n",
    "    'SMS': ['SMS', 'Message'],\n",
    "    'Scala': ['Scala'],\n",
    "    'Scientific': ['Scientific'],\n",
    "    'Science': ['Science'],\n",
    "    'Script': ['Script'],\n",
    "    'Search': ['Search'],\n",
    "    'SearchEngine-': ['SearchEngine'],\n",
    "    'Securi': ['ITSec', 'Security', 'InfoSec'],\n",
    "    'Server': ['Server'],\n",
    "    'ServerLess-': ['ServerLess'],\n",
    "    'Shell': ['Shell'],\n",
    "    'SideProject-': ['SideProject'],\n",
    "    'Smart': ['Smart'],\n",
    "    'Software': ['Software'],\n",
    "    'SSL': ['SSL', 'Encryption', 'ITSec', 'InfoSec'],\n",
    "    'Startup': ['Startup'],\n",
    "    'Static': ['Static'],\n",
    "    'Storage': ['Storage'],\n",
    "    'SubString-': ['SubString'], \n",
    "    'Suggestion': ['Recommendation', 'RecommendationEngine', 'RecommenderSystem'],\n",
    "    'System': ['System'],\n",
    "    'Teach': ['Teach'],\n",
    "    'TechDebt-': ['TechDebt'],\n",
    "    'TechicalDebt-': ['TechDebt'],\n",
    "    'Test': ['Test'],\n",
    "    'Tethering': ['Tethering'],\n",
    "    'Tool': ['Tool'],\n",
    "    'Tracking': ['Tracking'],\n",
    "    'UI': ['UI', 'UX', 'Design'],\n",
    "    'UX': ['UI', 'UX', 'Design'],\n",
    "    'VC': ['VC', 'FundRaising', 'Investor'],\n",
    "    'Visitor': [\"Visitor\"],\n",
    "    ' Web ': ['Web'], #Don't remove spaces\n",
    "    'WebApp-': ['WebApp', 'WebDevelopment'],\n",
    "    'WebDev-': ['WebDevelopment', 'WebApp'],\n",
    "    'WebDesign': [\"WebDesign\", 'Design', 'UI', 'UX'],\t\n",
    "    'WebDesigner': ['WebDesigner', \"WebDesign\", 'Design', 'UI', 'UX'],\n",
    "    'WebGL': ['WebGL'],\n",
    "    'WebSocket-': ['WebSocket', 'Web', 'Async'],\n",
    "    'Website': ['WebDevelopment', 'WebApp'],\n",
    "    'Wireframe': [\"Wireframe\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to FeedCrunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('consolidated.json') as f:\n",
    "    string = \"\"\n",
    "    for line in f:\n",
    "        string += line\n",
    "        \n",
    "data = json.loads(string)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "api_route = \"https://www.feedcrunch.io/api/1.0/authenticated/post/article/\"\n",
    "apikey_fc    = \"4377190f-24a5-4ef7-a761-a877bf6218e3\"\n",
    "api_url = api_route + apikey_fc + \"/\"\n",
    "\n",
    "restart_point = 1\n",
    "\n",
    "for index in range(restart_point, df.shape[0]):\n",
    "\n",
    "    fields = convert_dict_unicode2string(df.iloc[index].to_dict())\n",
    "    files = {}\n",
    "\n",
    "    data, headers = encode_multipart(fields, files)\n",
    "    request = urllib2.Request(api_url, data=data, headers=headers)\n",
    "\n",
    "    f = urllib2.urlopen(request)\n",
    "\n",
    "    tmp_rslt = f.read()\n",
    "    \n",
    "    try:\n",
    "        tmp_rslt = tmp_rslt.replace('true', '\"true\"')\n",
    "        tmp_rslt = tmp_rslt.replace('false', '\"false\"')\n",
    "\n",
    "        data_rslt = ast.literal_eval(tmp_rslt)\n",
    "        \n",
    "        \n",
    "        old_stdout = sys.stdout\n",
    "        log_file = open(\"article_posting.log\",\"a\")\n",
    "        sys.stdout = log_file\n",
    "        \n",
    "        if data_rslt[\"success\"] != \"true\":\n",
    "            print \"Error  : \" + str(index)\n",
    "        else:\n",
    "            print \"Success: \" + str(index)\n",
    "        \n",
    "        sys.stdout = old_stdout\n",
    "        log_file.close()\n",
    "            \n",
    "        time.sleep(5 * 60) # Sleep for 5 Minutes\n",
    "            \n",
    "    except Exception, e:\n",
    "        old_stdout = sys.stdout\n",
    "        log_file = open(\"article_posting.log\",\"a\")\n",
    "        sys.stdout = log_file\n",
    "        print \"#############\"\n",
    "        print \"Error at index : \" + str(index)\n",
    "        print \"An error occured in the process: \" + str(e)\n",
    "        sys.stdout = old_stdout\n",
    "        log_file.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
